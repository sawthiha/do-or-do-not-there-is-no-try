{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plot\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "matplotlib.rcParams.update({'font.size': 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden : [neurons of each specific layer]\n",
    "class NeuralNet:\n",
    "    def __init__(self, size_in, size_out, hidden, rate = 0.001, w_decay = 0, moment = 0.9, arbitary = 1e-25, av = None, av_ = None, loss = None, loss_ = None):\n",
    "        self.x = np.zeros((size_in, 1), dtype=np.float64)\n",
    "        self.y = np.zeros((size_out, 1), dtype=np.float64)\n",
    "        self.weight = []\n",
    "        self.weight_ = []\n",
    "        self.bias = []\n",
    "        self.bias_ = []\n",
    "        self.z = []\n",
    "        self.activations = []\n",
    "        self.E = []\n",
    "        self.delta = []\n",
    "        self.E_b = []\n",
    "        self.delta_b = []\n",
    "        \n",
    "        idx = 0\n",
    "        self.layer = len(hidden) + 1\n",
    "        n = self.layer - 1\n",
    "        self.weight.append(np.random.rand(hidden[idx], size_in) * np.sqrt(2 / size_in))\n",
    "        self.weight_.append(np.zeros((hidden[idx], size_in)))\n",
    "        self.E.append(np.zeros((hidden[idx], size_in)))\n",
    "        self.delta.append(np.zeros((hidden[idx], size_in)))\n",
    "        self.bias.append(np.random.rand(hidden[idx], 1))\n",
    "        self.bias_.append(np.zeros((hidden[idx], 1)))\n",
    "        self.E_b.append(np.zeros((hidden[idx], 1)))\n",
    "        self.delta_b.append(np.zeros((hidden[idx], 1)))\n",
    "        self.activations.append(np.zeros((hidden[idx], 1)))\n",
    "        self.z.append(np.zeros((hidden[idx], 1)))\n",
    "        idx += 1\n",
    "        \n",
    "        while idx < n:\n",
    "            self.weight.append(np.random.rand(hidden[idx], hidden[idx - 1]) * np.sqrt(2 / hidden[idx - 1]))\n",
    "            self.weight_.append(np.zeros((hidden[idx], hidden[idx - 1])))\n",
    "            self.E.append(np.zeros((hidden[idx], hidden[idx - 1])))\n",
    "            self.delta.append(np.zeros((hidden[idx], hidden[idx - 1])))\n",
    "            self.bias.append(np.random.rand(hidden[idx], 1))\n",
    "            self.bias_.append(np.zeros((hidden[idx], 1)))\n",
    "            self.E_b.append(np.zeros((hidden[idx], 1)))\n",
    "            self.delta_b.append(np.zeros((hidden[idx], 1)))\n",
    "            self.z.append(np.zeros((hidden[idx], 1)))\n",
    "            self.activations.append(np.zeros((hidden[idx], 1)))\n",
    "            idx += 1\n",
    "        \n",
    "        self.weight.append(np.random.rand(size_out, hidden[idx - 1]) * np.sqrt(2 / hidden[idx - 1]))\n",
    "        self.weight_.append(np.zeros((size_out, hidden[idx - 1])))\n",
    "        self.E.append(np.zeros((size_out, hidden[idx - 1])))\n",
    "        self.delta.append(np.zeros((size_out, hidden[idx - 1])))\n",
    "        self.bias.append(np.random.rand(size_out, 1))\n",
    "        self.bias_.append(np.zeros((size_out, 1)))\n",
    "        self.E_b.append(np.zeros((size_out, 1)))\n",
    "        self.delta_b.append(np.zeros((size_out, 1)))\n",
    "        self.activations.append(np.zeros((size_out, 1)))\n",
    "        self.z.append(np.zeros((size_out, 1)))\n",
    "        \n",
    "        self.rate = rate\n",
    "        self.w_decay = w_decay\n",
    "        self.moment = moment\n",
    "        self.arbitary = arbitary\n",
    "        \n",
    "        if(hasattr(av, '__call__')):\n",
    "            self.activate = av\n",
    "        if(hasattr(av_, '__call__')):\n",
    "            self.activate_ = av_\n",
    "        if(hasattr(loss, '__call__')):\n",
    "            self.cost = loss\n",
    "        if(hasattr(loss_, '__call__')):\n",
    "            self.cost_ = loss_\n",
    "    \n",
    "    def activate(self, x):\n",
    "        return (1 - np.exp(-(x * 2))) / (1 + np.exp(-(x * 2)))\n",
    "    \n",
    "    def activate_(self, x):\n",
    "        return 1 - np.square(self.activate(x))   \n",
    "    \n",
    "    def cost(self, y):\n",
    "        return (self.y - y) ** 2\n",
    "    \n",
    "    def cost_(self, y):\n",
    "        return (self.y - y) * 2\n",
    "    \n",
    "    def feed(self, x):\n",
    "        self.x[:] = x.reshape((x.shape[0], 1))\n",
    "        idx = 0\n",
    "        n = self.layer - 1\n",
    "        self.z[idx] = self.weight[idx].dot(self.x) + self.bias[idx]\n",
    "        self.activations[idx] = self.activate(self.z[idx])\n",
    "        idx += 1\n",
    "        \n",
    "        while idx < n:\n",
    "            self.z[idx] = self.weight[idx].dot(self.activations[idx - 1]) + self.bias[idx]\n",
    "            self.activations[idx] = self.activate(self.z[idx])\n",
    "            idx += 1\n",
    "        \n",
    "        self.z[idx] = self.weight[idx].dot(self.activations[idx - 1]) + self.bias[idx]\n",
    "        self.y = self.activate(self.z[idx])\n",
    "        \n",
    "    def propagate(self, y):\n",
    "        y = y.reshape((y.shape[0], 1))\n",
    "        idx = self.layer - 1\n",
    "        i_ = self.activate_(self.z[idx]) * self.cost_(y)\n",
    "        self.weight_[idx] = i_.dot(self.activations[idx - 1].T)\n",
    "        self.bias_[idx] = i_\n",
    "        c_ = self.weight[idx].T.dot(i_)\n",
    "        idx -= 1\n",
    "        \n",
    "        while idx > 0:\n",
    "            i_ = self.activate_(self.z[idx]) * c_\n",
    "            self.weight_[idx] = i_.dot(self.activations[idx - 1].T)\n",
    "            self.bias_[idx] = i_\n",
    "            c_ = self.weight[idx].T.dot(i_)\n",
    "            idx -= 1\n",
    "        \n",
    "        i_ = self.activate_(self.z[idx]) * c_\n",
    "        self.weight_[idx] = i_.dot(self.x.T)\n",
    "        self.bias_[idx] = i_\n",
    "        \n",
    "        while idx < self.layer:\n",
    "            self.weight_[idx] += self.weight[idx] * self.w_decay\n",
    "            w_rate, b_rate = self.adadelta(idx)\n",
    "            self.weight[idx] -= w_rate * self.weight_[idx]\n",
    "            self.bias[idx] -= b_rate * self.bias_[idx]\n",
    "            idx += 1\n",
    "        \n",
    "    def heetal_w(self, cur, prev, com):\n",
    "        return np.random.randn(com, cur) * np.sqrt(2 / prev)\n",
    "        \n",
    "    def adadelta(self, layer):\n",
    "        self.E[layer] = self.moment * self.E[layer] + (1 - self.moment) * (self.weight_[layer] ** 2)\n",
    "        delta = self.rate * self.weight_[layer]\n",
    "        delta /= np.sqrt(self.E[layer] + self.arbitary)\n",
    "        old_delta = self.delta[layer]\n",
    "        self.delta[layer] = self.moment * self.delta[layer] + (1 - self.moment) * (delta ** 2)\n",
    "        \n",
    "        self.E_b[layer] = self.moment * self.E_b[layer] + (1 - self.moment) * (self.bias_[layer] ** 2)\n",
    "        delta = self.rate * self.bias_[layer]\n",
    "        delta /= np.sqrt(self.E_b[layer] + self.arbitary)\n",
    "        old_delta_b = self.delta_b[layer]\n",
    "        self.delta_b[layer] = self.moment * self.delta_b[layer] + (1 - self.moment) * (delta ** 2)\n",
    "        \n",
    "        w_rate = np.sqrt(old_delta + self.arbitary)\n",
    "        w_rate /= np.sqrt(self.E[layer] + self.arbitary)\n",
    "        \n",
    "        b_rate = np.sqrt(old_delta_b + self.arbitary)\n",
    "        b_rate /= np.sqrt(self.E_b[layer] + self.arbitary)\n",
    "        \n",
    "        return w_rate, b_rate\n",
    "    \n",
    "        \n",
    "    def result(self):\n",
    "        return self.y\n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def tanh(x):\n",
    "    return (2 / (1 + np.exp(-2 * x))) - 1\n",
    "\n",
    "def tanh_(x):\n",
    "    return 1 - np.square(tanh(x))\n",
    "\n",
    "def relu(x, a = 0.01):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def relu_(x, a = 0.01):\n",
    "    return 1 * (x > 0)\n",
    "    \n",
    "def soe(dif):\n",
    "    return np.square(dif)\n",
    "\n",
    "def soe_(dif):\n",
    "    return 2 * dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/data-science-group-iitr/logistic-regression-simplified-9b4efe801389\n",
    "#https://stackoverflow.com/questions/3985619/how-to-calculate-a-logistic-sigmoid-function-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plot.subplots(nrows=2, figsize=(12,9))\n",
    "\n",
    "ax0.plot(x, sg, linewidth=3)\n",
    "ax0.spines['bottom'].set_position('zero')\n",
    "ax0.set_title('Sigmoid')\n",
    "\n",
    "ax1.plot(x, sg_, linewidth=3)\n",
    "ax1.spines['bottom'].set_position('zero')\n",
    "ax1.set_title('dSigmoid')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['left'].set_smart_bounds(True)\n",
    "    ax.spines['bottom'].set_smart_bounds(True)\n",
    "    ax.get_yaxis().tick_left()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plot.subplots(nrows=2, figsize=(12,9))\n",
    "\n",
    "ax0.plot(x, th, linewidth=3)\n",
    "ax0.spines['bottom'].set_position('center')\n",
    "ax0.set_title('Tanh')\n",
    "\n",
    "ax1.plot(x, th_, linewidth=3)\n",
    "ax1.spines['bottom'].set_position('zero')\n",
    "ax1.set_title('dTanh')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['left'].set_smart_bounds(True)\n",
    "    ax.spines['bottom'].set_smart_bounds(True)\n",
    "    ax.get_yaxis().tick_left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plot.subplots(nrows=2, figsize=(12,9))\n",
    "\n",
    "ax0.plot(x, square, linewidth=3)\n",
    "ax0.spines['bottom'].set_position('zero')\n",
    "ax0.set_title('SoE')\n",
    "\n",
    "ax1.plot(x, square_, linewidth=3)\n",
    "ax1.spines['bottom'].set_position('zero')\n",
    "ax1.set_title('dSoE')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['left'].set_smart_bounds(True)\n",
    "    ax.spines['bottom'].set_smart_bounds(True)\n",
    "    ax.get_yaxis().tick_left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax0, ax1) = plot.subplots(nrows=2, figsize=(12,9))\n",
    "\n",
    "ax0.plot(x, r, linewidth=3)\n",
    "ax0.spines['bottom'].set_position('zero')\n",
    "ax0.set_title('ReLU')\n",
    "\n",
    "ax1.plot(x, r_, linewidth=3)\n",
    "ax1.spines['bottom'].set_position('zero')\n",
    "ax1.set_title('dReLU')\n",
    "\n",
    "for ax in (ax0, ax1):\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.get_xaxis().tick_bottom()\n",
    "    ax.spines['left'].set_position('zero')\n",
    "    ax.spines['left'].set_smart_bounds(True)\n",
    "    ax.spines['bottom'].set_smart_bounds(True)\n",
    "    ax.get_yaxis().tick_left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('tmp/mnist.pkl.gz', 'rb') as file:\n",
    "    train_set, valid_set, test_set = pickle.load(file, encoding='iso-8859-1')\n",
    "train_x, train_y = train_set\n",
    "valid_x, valid_y = valid_set\n",
    "test_x, test_y = test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = np.zeros((10, 1))\n",
    "n = 1 # Only one training? If not change it\n",
    "plot_size = (train_x.shape[0] * n) + 1\n",
    "xaxis = np.arange(0, plot_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8314049564319982"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_sigmoid = NeuralNet(784, 10, [25, 25], av = sigmoid, av_ = sigmoid_)\n",
    "AdaDelta(ann_sigmoid)\n",
    "cost_sinh, w_sinh, b_sinh = train(ann_sigmoid, train_x, train_y)\n",
    "validate(ann_sigmoid, valid_x, valid_y)\n",
    "validate(ann_sigmoid, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8494083858597443"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_tanh = NeuralNet(784, 10, [25, 25])\n",
    "AdaDelta(ann_tanh)\n",
    "cost_tanh, w_tanh, b_tanh = train(ann_tanh, train_x, train_y)\n",
    "validate(ann_tanh, valid_x, valid_y)\n",
    "validate(ann_tanh, test_x, test_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_relu = NeuralNet(784, 10, [25, 25], w_decay=0.0001, av = relu, av_ = relu_)\n",
    "AdaDelta(ann_relu)\n",
    "cost_relu, w_relu, b_relu = train(ann_relu, train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8308043695709708"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 68\n",
    "#train(ann_relu, valid_x, valid_y)\n",
    "validate(ann_relu, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ann, x, y, n = 1):\n",
    "    i = 0\n",
    "    epoch = 1\n",
    "    out = np.zeros(10)\n",
    "    plot_size = (x.shape[0] * n) + 1\n",
    "    costs = np.zeros(plot_size)\n",
    "    w_ = np.zeros(plot_size)\n",
    "    b_ = np.zeros(plot_size)\n",
    "    while i < n:\n",
    "        idx = x.shape[0] - 1\n",
    "        while idx > -1:\n",
    "            ann.feed(x[idx])\n",
    "            out[y[idx]] = 1\n",
    "            costs[epoch] = ann.cost(out).sum()\n",
    "            ann.propagate(out)\n",
    "            w_[epoch] = ann.weight_[2][0][0]\n",
    "            b_[epoch] = ann.bias_[2][0][0]\n",
    "            out[y[idx]] = 0\n",
    "            idx -= 1\n",
    "            epoch += 1\n",
    "        i += 1\n",
    "    return (w_, b_, costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(ann, x, y):\n",
    "    idx = x.shape[0] - 1\n",
    "    sum = 0.0\n",
    "    out = np.zeros((10, 1))\n",
    "    while idx > -1:\n",
    "        ann.feed(x[idx])\n",
    "        out[y[idx]] = 1\n",
    "        sum += ann.cost(out.reshape((out.shape[0], 1))).sum()\n",
    "        out[y[idx]] = 0\n",
    "        idx -=  1\n",
    "    sum /= x.shape[0]\n",
    "    return 1 - sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-463a7c81299a>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-41-463a7c81299a>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    y[test_y[idx] = 1\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "idx = test_x.shape[0] - 1\n",
    "sum = 0.0\n",
    "while idx > -1:\n",
    "    ann_sigmoid.feed(test_x[idx])\n",
    "    y[test_y[idx]] = 1\n",
    "    sum = sum + ann.cost(y).sum()\n",
    "    y[test_y[idx]][0] = 0\n",
    "    idx = idx - 1\n",
    "sum = sum / test_x.shape[0]\n",
    "print(1 - sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax0 = plot.subplots(nrows=1, figsize=(12, 9))\n",
    "ax0.plot(xaxis[::100], cost_sinh[::100], linewidth=1, label='sinh with 0.1 Learning Rate')\n",
    "ax0.plot(xaxis[::100], cost_tanh[::100], linewidth=1, label='tanh with 0.1 Learning Rate')\n",
    "ax0.plot(xaxis[::100], cost_relu[::100], linewidth=1, label='relu with 0.0001 Learning Rate')\n",
    "ax0.spines['bottom'].set_position('zero')\n",
    "ax0.set_title('Traning Costs')\n",
    "ax0.legend()\n",
    "\n",
    "ax0.spines['top'].set_visible(False)\n",
    "ax0.spines['right'].set_visible(False)\n",
    "ax0.get_xaxis().tick_bottom()\n",
    "ax0.spines['left'].set_smart_bounds(True)\n",
    "ax0.spines['left'].set_position('zero')\n",
    "ax0.spines['bottom'].set_smart_bounds(True)\n",
    "ax0.get_yaxis().tick_left()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use(\"TkAgg\")\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg, NavigationToolbar2TkAgg\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.pyplot as plot\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n",
    "\n",
    "class FixedOrderFormatter(ScalarFormatter):\n",
    "    \"\"\"Formats axis ticks using scientific notation with a constant order of \n",
    "    magnitude\"\"\"\n",
    "    def __init__(self, order_of_mag=0, useOffset=True, useMathText=False):\n",
    "        self._order_of_mag = order_of_mag\n",
    "        ScalarFormatter.__init__(self, useOffset=useOffset, \n",
    "                                 useMathText=useMathText)\n",
    "    def _set_orderOfMagnitude(self, range):\n",
    "        \"\"\"Over-riding this to avoid having orderOfMagnitude reset elsewhere\"\"\"\n",
    "        self.orderOfMagnitude = self._order_of_mag\n",
    "\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "\n",
    "\n",
    "fig = plot.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "rpr_dW = w_sinh[:60]\n",
    "rpr_dB = b_sinh[:60]\n",
    "rpr_costs = cost_sinh[:60]\n",
    "\n",
    "for k in range(0, rpr_dW.size, 1):\n",
    "    ax.plot(rpr_dW[:k], rpr_dB[:k], rpr_costs[:k])\n",
    "    \n",
    "    #ax.set_xlim3d(-0.4, 0.4)\n",
    "    #ax.set_ylim3d(-0.4, 0.4)\n",
    "    #ax.set_zlim3d(0, 100)\n",
    "    ax.xaxis.set_label_text('dW')\n",
    "    ax.yaxis.set_label_text('dB')\n",
    "    ax.zaxis.set_label_text('Costs')\n",
    "    ax.xaxis.label.set_fontsize(12)\n",
    "    ax.yaxis.label.set_fontsize(12)\n",
    "    ax.zaxis.label.set_fontsize(12)\n",
    "    ax.xaxis.set_major_formatter(FixedOrderFormatter(0))\n",
    "    ax.yaxis.set_major_formatter(FixedOrderFormatter(0))\n",
    "    ax.zaxis.set_major_formatter(FixedOrderFormatter(0))\n",
    "    plot.draw()\n",
    "    plot.pause(0.05)\n",
    "    if k != rpr_dW.size - 1:\n",
    "        ax.cla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot.figure(figsize=(12, 9))\n",
    "ax = fig.gca(projection = '3d')\n",
    "ax.set_title('Gradient Plane')\n",
    "#ax.plot(w_tanh[::100], b_tanh[::100], np.vstack((cost_relu[::100], xaxis[::100])), label = 'Tanh')\n",
    "#ax.plot(w_sinh[::100], b_sinh[::100], np.vstack((cost_relu[::100], xaxis[::100])), label = 'Sinh')\n",
    "ax.plot_wireframe(w_relu[::100], b_relu[::100], np.vstack((cost_relu[::100], xaxis[::100])), label = 'Relu')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roll, year, rc, semester, external, prev_performance\n",
    "# activities, internal, external\n",
    "\n",
    "activities = roll + academic_progress + attendance + semester + external + prev_performance\n",
    "internal = roll + acdaemic_progress + attendance + semester + external + prev_performance\n",
    "external = roll + academic_progress + attendance + semester + external + prev_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 6)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nero = NeuralNet(6, 3, [25, 25], rate=0.01, w_decay= 0.1, av= relu, av_=relu_)\n",
    "nero.weight_[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
